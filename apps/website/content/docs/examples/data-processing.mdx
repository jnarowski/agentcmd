---
title: Data Processing
description: AI-powered data processing workflows for transformation, analysis, and enrichment
---

import { Callout } from 'fumadocs-ui/components/callout';
import { Tabs, Tab } from 'fumadocs-ui/components/tabs';

Build workflows that leverage AI for data transformation, analysis, and enrichment. Learn to process CSV files, generate reports, and extract insights from unstructured data.

## Overview

AI-powered data processing workflows typically follow this pattern:

1. **Ingest** - Load and validate data from various sources
2. **Transform** - Clean, normalize, and structure data
3. **Analyze** - Extract insights using AI
4. **Output** - Generate reports or export processed data

## Simple Example

Process a CSV file and generate insights:

```typescript
import { defineWorkflow } from 'agentcmd-workflows';
import { z } from 'zod';

export default defineWorkflow({
  id: 'analyze-csv-simple',
  phases: ['load', 'analyze']
}, async ({ event, step }) => {
  const { workingDir } = event.data;

  let csvData: string;

  await step.phase('load', async () => {
    const result = await step.cli('read-csv', {
      command: 'cat data/sales.csv',
      cwd: workingDir
    });
    csvData = result.data.stdout || '';
  });

  await step.phase('analyze', async () => {
    const analysisSchema = z.object({
      totalSales: z.number(),
      topProducts: z.array(z.string()),
      insights: z.array(z.string())
    });

    const result = await step.ai('analyze-sales', {
      prompt: `Analyze this sales data and return JSON:
${csvData}

Return: { totalSales, topProducts, insights }`,
      schema: analysisSchema
    });

    step.log('Analysis:', result.data);
  });
});
```

## Focused Example

Transform JSON data with structured output:

```typescript
import { defineWorkflow } from 'agentcmd-workflows';
import { z } from 'zod';

const customerSchema = z.object({
  id: z.string(),
  name: z.string(),
  email: z.string(),
  segment: z.enum(['enterprise', 'mid-market', 'smb']),
  risk: z.enum(['high', 'medium', 'low']),
  recommendations: z.array(z.string())
});

type Customer = z.infer<typeof customerSchema>;

export default defineWorkflow({
  id: 'enrich-customers',
  phases: ['load', 'enrich', 'export']
}, async ({ event, step }) => {
  const { workingDir } = event.data;

  interface Context {
    customers?: Customer[];
    enrichedData?: Customer[];
  }
  const ctx: Context = {};

  await step.phase('load', async () => {
    const result = await step.cli('load-customers', {
      command: 'cat data/customers.json',
      cwd: workingDir
    });

    ctx.customers = JSON.parse(result.data.stdout || '[]');
    step.log(`Loaded ${ctx.customers.length} customers`);
  });

  await step.phase('enrich', async () => {
    if (!ctx.customers) throw new Error('No customers loaded');

    const enrichedCustomers: Customer[] = [];

    // Process in batches of 10
    for (let i = 0; i < ctx.customers.length; i += 10) {
      const batch = ctx.customers.slice(i, i + 10);

      const result = await step.ai(`enrich-batch-${i}`, {
        prompt: `Analyze these customers and add segmentation and risk scoring:

${JSON.stringify(batch, null, 2)}

For each customer, determine:
1. segment (enterprise/mid-market/smb)
2. risk level (high/medium/low)
3. recommendations (array of actionable recommendations)

Return array of enriched customer objects.`,
        schema: z.array(customerSchema)
      });

      enrichedCustomers.push(...result.data);
      step.log(`Enriched batch ${i / 10 + 1}: ${result.data.length} customers`);
    }

    ctx.enrichedData = enrichedCustomers;
  });

  await step.phase('export', async () => {
    if (!ctx.enrichedData) throw new Error('No enriched data');

    // Export as JSON
    await step.artifact('enriched-customers', {
      name: 'enriched-customers.json',
      type: 'text',
      content: JSON.stringify(ctx.enrichedData, null, 2)
    });

    // Generate summary report
    const summary = {
      total: ctx.enrichedData.length,
      bySegment: {
        enterprise: ctx.enrichedData.filter(c => c.segment === 'enterprise').length,
        midMarket: ctx.enrichedData.filter(c => c.segment === 'mid-market').length,
        smb: ctx.enrichedData.filter(c => c.segment === 'smb').length
      },
      byRisk: {
        high: ctx.enrichedData.filter(c => c.risk === 'high').length,
        medium: ctx.enrichedData.filter(c => c.risk === 'medium').length,
        low: ctx.enrichedData.filter(c => c.risk === 'low').length
      }
    };

    await step.artifact('summary', {
      name: 'summary.json',
      type: 'text',
      content: JSON.stringify(summary, null, 2)
    });

    step.log('Processing complete:', summary);
  });

  return {
    success: true,
    processed: ctx.enrichedData?.length || 0
  };
});
```

## Production Example

Complete ETL pipeline with validation, error handling, and monitoring:

```typescript
import { defineWorkflow, defineSchema } from 'agentcmd-workflows';
import { z } from 'zod';

// Input schema
const argsSchema = defineSchema({
  type: 'object',
  properties: {
    inputFile: { type: 'string' },
    outputFormat: { enum: ['json', 'csv', 'markdown'] },
    batchSize: { type: 'number' },
    skipValidation: { type: 'boolean' }
  },
  required: ['inputFile', 'outputFormat']
});

type WorkflowArgs = {
  inputFile: string;
  outputFormat: 'json' | 'csv' | 'markdown';
  batchSize?: number;
  skipValidation?: boolean;
};

// Data schemas
const rawRecordSchema = z.object({
  id: z.string(),
  timestamp: z.string(),
  event: z.string(),
  metadata: z.record(z.unknown()).optional()
});

const enrichedRecordSchema = z.object({
  id: z.string(),
  timestamp: z.string(),
  event: z.string(),
  category: z.enum(['user', 'system', 'integration', 'error']),
  severity: z.enum(['critical', 'high', 'medium', 'low', 'info']),
  summary: z.string(),
  actionItems: z.array(z.string()),
  metadata: z.record(z.unknown()).optional()
});

type RawRecord = z.infer<typeof rawRecordSchema>;
type EnrichedRecord = z.infer<typeof enrichedRecordSchema>;

export default defineWorkflow(
  {
    id: 'data-processing-etl',
    name: 'Data Processing ETL Pipeline',
    description: 'Extract, transform, and load data with AI-powered enrichment',
    phases: [
      { id: 'validate', label: 'Validate Input' },
      { id: 'extract', label: 'Extract Data' },
      { id: 'transform', label: 'Transform & Enrich' },
      { id: 'quality', label: 'Quality Check' },
      { id: 'load', label: 'Load Output' }
    ],
    argsSchema
  },
  async ({ event, step }) => {
    const args = event.data.args as unknown as WorkflowArgs;
    const { workingDir } = event.data;
    const {
      inputFile,
      outputFormat,
      batchSize = 20,
      skipValidation = false
    } = args;

    // Workflow context
    interface Context {
      rawData?: RawRecord[];
      enrichedData?: EnrichedRecord[];
      validationErrors?: string[];
      stats?: {
        total: number;
        processed: number;
        failed: number;
        batchCount: number;
      };
      outputPath?: string;
    }
    const ctx: Context = {
      stats: {
        total: 0,
        processed: 0,
        failed: 0,
        batchCount: 0
      }
    };

    // ========================================================================
    // VALIDATE PHASE
    // ========================================================================

    if (!skipValidation) {
      await step.phase('validate', async () => {
        await step.annotation('validate-start', {
          message: `Validating input file: ${inputFile}`
        });

        // Check file exists
        const checkResult = await step.cli('check-file', {
          command: `test -f "${inputFile}" && echo "exists" || echo "missing"`,
          cwd: workingDir
        });

        if (checkResult.data.stdout?.trim() !== 'exists') {
          throw new Error(`Input file not found: ${inputFile}`);
        }

        // Check file size
        const sizeResult = await step.cli('check-size', {
          command: `wc -l "${inputFile}" | awk '{print $1}'`,
          cwd: workingDir
        });

        const lineCount = parseInt(sizeResult.data.stdout?.trim() || '0');
        step.log(`Input file has ${lineCount} lines`);

        if (lineCount === 0) {
          throw new Error('Input file is empty');
        }

        if (lineCount > 10000) {
          step.log.warn(`Large file detected (${lineCount} lines) - processing may take time`);
        }
      });
    } else {
      step.log('Skipping validation per workflow arguments');
    }

    // ========================================================================
    // EXTRACT PHASE
    // ========================================================================

    await step.phase('extract', async () => {
      await step.annotation('extract-start', {
        message: 'Extracting data from source'
      });

      const readResult = await step.cli('read-file', {
        command: `cat "${inputFile}"`,
        cwd: workingDir
      });

      try {
        // Parse JSON lines or JSON array
        const content = readResult.data.stdout || '';
        const lines = content.trim().split('\n');

        if (lines[0]?.startsWith('[')) {
          // JSON array
          ctx.rawData = JSON.parse(content);
        } else {
          // JSON lines
          ctx.rawData = lines
            .filter(line => line.trim())
            .map(line => JSON.parse(line));
        }

        ctx.stats!.total = ctx.rawData.length;
        step.log(`Extracted ${ctx.stats!.total} records`);

        // Validate schema
        ctx.validationErrors = [];
        ctx.rawData = ctx.rawData.filter((record, index) => {
          try {
            rawRecordSchema.parse(record);
            return true;
          } catch (error) {
            ctx.validationErrors!.push(`Record ${index}: ${error}`);
            ctx.stats!.failed++;
            return false;
          }
        });

        if (ctx.validationErrors.length > 0) {
          step.log.warn(`${ctx.validationErrors.length} validation errors`);
          await step.artifact('validation-errors', {
            name: 'validation-errors.txt',
            type: 'text',
            content: ctx.validationErrors.join('\n')
          });
        }

      } catch (error) {
        step.log.error('Failed to parse input:', error);
        throw new Error(`Invalid input format: ${error}`);
      }
    });

    // ========================================================================
    // TRANSFORM PHASE
    // ========================================================================

    await step.phase('transform', async () => {
      if (!ctx.rawData || ctx.rawData.length === 0) {
        throw new Error('No valid data to process');
      }

      await step.annotation('transform-start', {
        message: `Transforming ${ctx.rawData.length} records in batches of ${batchSize}`
      });

      ctx.enrichedData = [];
      const batches = Math.ceil(ctx.rawData.length / batchSize);
      ctx.stats!.batchCount = batches;

      for (let i = 0; i < batches; i++) {
        const start = i * batchSize;
        const end = Math.min(start + batchSize, ctx.rawData.length);
        const batch = ctx.rawData.slice(start, end);

        step.log(`Processing batch ${i + 1}/${batches} (${batch.length} records)`);

        try {
          const result = await step.ai<z.infer<typeof enrichedRecordSchema>[]>(
            `enrich-batch-${i}`,
            {
              prompt: `Analyze these event records and enrich them with categorization and insights:

${JSON.stringify(batch, null, 2)}

For each record, add:
1. category: Classify as 'user', 'system', 'integration', or 'error'
2. severity: Rate as 'critical', 'high', 'medium', 'low', or 'info'
3. summary: One-sentence summary of what happened
4. actionItems: Array of recommended actions (empty if none needed)

Return the complete enriched records as JSON array.`,
              schema: z.array(enrichedRecordSchema),
              provider: 'anthropic',
              model: 'claude-sonnet-4-5-20250929',
              temperature: 0.3
            }
          );

          ctx.enrichedData.push(...result.data);
          ctx.stats!.processed += result.data.length;

          step.log(`Batch ${i + 1} complete: enriched ${result.data.length} records`);

        } catch (error) {
          step.log.error(`Batch ${i + 1} failed:`, error);

          // Add failed batch to error log
          await step.artifact(`batch-${i}-error`, {
            name: `batch-${i}-error.json`,
            type: 'text',
            content: JSON.stringify({
              batchIndex: i,
              records: batch,
              error: String(error)
            }, null, 2)
          });

          ctx.stats!.failed += batch.length;
          // Continue with next batch
        }

        // Progress annotation every 5 batches
        if ((i + 1) % 5 === 0) {
          await step.annotation(`progress-${i}`, {
            message: `Processed ${ctx.stats!.processed}/${ctx.stats!.total} records`
          });
        }
      }

      step.log('Transformation complete:', {
        processed: ctx.stats!.processed,
        failed: ctx.stats!.failed,
        total: ctx.stats!.total
      });
    });

    // ========================================================================
    // QUALITY CHECK PHASE
    // ========================================================================

    await step.phase('quality', async () => {
      if (!ctx.enrichedData || ctx.enrichedData.length === 0) {
        throw new Error('No enriched data to check');
      }

      await step.annotation('quality-start', {
        message: 'Running quality checks'
      });

      // Check for completeness
      const incomplete = ctx.enrichedData.filter(record =>
        !record.category || !record.severity || !record.summary
      );

      if (incomplete.length > 0) {
        step.log.warn(`${incomplete.length} records incomplete`);
        await step.artifact('incomplete-records', {
          name: 'incomplete-records.json',
          type: 'text',
          content: JSON.stringify(incomplete, null, 2)
        });
      }

      // Generate quality report
      const qualityReport = {
        totalRecords: ctx.enrichedData.length,
        incomplete: incomplete.length,
        byCategory: {
          user: ctx.enrichedData.filter(r => r.category === 'user').length,
          system: ctx.enrichedData.filter(r => r.category === 'system').length,
          integration: ctx.enrichedData.filter(r => r.category === 'integration').length,
          error: ctx.enrichedData.filter(r => r.category === 'error').length
        },
        bySeverity: {
          critical: ctx.enrichedData.filter(r => r.severity === 'critical').length,
          high: ctx.enrichedData.filter(r => r.severity === 'high').length,
          medium: ctx.enrichedData.filter(r => r.severity === 'medium').length,
          low: ctx.enrichedData.filter(r => r.severity === 'low').length,
          info: ctx.enrichedData.filter(r => r.severity === 'info').length
        },
        withActionItems: ctx.enrichedData.filter(r => r.actionItems.length > 0).length
      };

      await step.artifact('quality-report', {
        name: 'quality-report.json',
        type: 'text',
        content: JSON.stringify(qualityReport, null, 2)
      });

      step.log('Quality check complete:', qualityReport);
    });

    // ========================================================================
    // LOAD PHASE
    // ========================================================================

    await step.phase('load', async () => {
      if (!ctx.enrichedData) throw new Error('No data to output');

      await step.annotation('load-start', {
        message: `Generating output in ${outputFormat} format`
      });

      let outputContent: string;
      let outputExt: string;

      switch (outputFormat) {
        case 'json':
          outputContent = JSON.stringify(ctx.enrichedData, null, 2);
          outputExt = 'json';
          break;

        case 'csv':
          const headers = ['id', 'timestamp', 'event', 'category', 'severity', 'summary', 'actionItems'];
          const csvRows = [
            headers.join(','),
            ...ctx.enrichedData.map(record =>
              headers.map(h => {
                const value = record[h as keyof EnrichedRecord];
                return Array.isArray(value)
                  ? `"${value.join('; ')}"`
                  : `"${String(value).replace(/"/g, '""')}"`;
              }).join(',')
            )
          ];
          outputContent = csvRows.join('\n');
          outputExt = 'csv';
          break;

        case 'markdown':
          const mdRows = [
            '# Data Processing Report',
            '',
            `Processed: ${new Date().toISOString()}`,
            `Total Records: ${ctx.enrichedData.length}`,
            '',
            '## Records',
            '',
            ...ctx.enrichedData.map(record => `
### ${record.event}

- **ID**: ${record.id}
- **Timestamp**: ${record.timestamp}
- **Category**: ${record.category}
- **Severity**: ${record.severity}
- **Summary**: ${record.summary}
${record.actionItems.length > 0 ? `- **Action Items**:\n${record.actionItems.map(a => `  - ${a}`).join('\n')}` : ''}
`)
          ];
          outputContent = mdRows.join('\n');
          outputExt = 'md';
          break;

        default:
          throw new Error(`Unsupported output format: ${outputFormat}`);
      }

      const outputFileName = `enriched-data.${outputExt}`;
      ctx.outputPath = outputFileName;

      await step.artifact('output', {
        name: outputFileName,
        type: 'text',
        content: outputContent,
        description: `Enriched data in ${outputFormat} format`
      });

      step.log(`Output generated: ${outputFileName} (${outputContent.length} bytes)`);
    });

    // ========================================================================
    // RETURN RESULTS
    // ========================================================================

    return {
      success: true,
      stats: ctx.stats,
      outputFile: ctx.outputPath,
      qualityIssues: ctx.validationErrors?.length || 0
    };
  }
);
```

## Running the Workflow

<Tabs items={['CLI', 'Programmatic']}>
<Tab value="CLI">
```bash
# Process JSON file to CSV
agentcmd run data-processing-etl \
  --inputFile "./data/events.json" \
  --outputFormat "csv" \
  --batchSize 20

# Large file processing
agentcmd run data-processing-etl \
  --inputFile "./data/large-dataset.json" \
  --outputFormat "json" \
  --batchSize 50 \
  --skipValidation false

# Generate markdown report
agentcmd run data-processing-etl \
  --inputFile "./data/audit-logs.json" \
  --outputFormat "markdown"
```
</Tab>
<Tab value="Programmatic">
```typescript
import { runWorkflow } from 'agentcmd-workflows';
import etlWorkflow from './.agent/workflows/definitions/data-processing-etl';

const result = await runWorkflow(etlWorkflow, {
  args: {
    inputFile: './data/events.json',
    outputFormat: 'json',
    batchSize: 20
  },
  projectPath: process.cwd()
});

console.log('Processed:', result.stats.processed);
```
</Tab>
</Tabs>

## Best Practices

### Batch Processing for Large Datasets

Always process large datasets in batches:

```typescript
const batchSize = 20; // Optimal for most AI models
const batches = Math.ceil(data.length / batchSize);

for (let i = 0; i < batches; i++) {
  const batch = data.slice(i * batchSize, (i + 1) * batchSize);

  const result = await step.ai(`batch-${i}`, {
    prompt: `Process this batch: ${JSON.stringify(batch)}`,
    schema: outputSchema
  });

  processedData.push(...result.data);
}
```

### Use Zod for Validation

Define strict schemas for data quality:

```typescript
import { z } from 'zod';

const recordSchema = z.object({
  id: z.string().uuid(),
  email: z.string().email(),
  age: z.number().min(0).max(120),
  status: z.enum(['active', 'inactive'])
});

// Validate during extraction
const validated = data.filter((record) => {
  try {
    recordSchema.parse(record);
    return true;
  } catch {
    return false;
  }
});
```

### Monitor Progress

Use annotations for long-running workflows:

```typescript
const totalBatches = 100;

for (let i = 0; i < totalBatches; i++) {
  // Process batch...

  if (i % 10 === 0) {
    await step.annotation(`progress-${i}`, {
      message: `Processed ${i}/${totalBatches} batches (${Math.round(i/totalBatches * 100)}%)`
    });
  }
}
```

### Handle Failures Gracefully

Continue processing on batch failures:

```typescript
for (const batch of batches) {
  try {
    const result = await step.ai('process', { ... });
    successCount += result.data.length;
  } catch (error) {
    failedBatches.push({ batch, error });
    // Continue with next batch
  }
}

if (failedBatches.length > 0) {
  await step.artifact('failed-batches', {
    name: 'failures.json',
    type: 'text',
    content: JSON.stringify(failedBatches, null, 2)
  });
}
```

## Common Pitfalls

<Callout type="warning">
**Avoid these mistakes:**

1. **Processing entire dataset at once** - Always use batching for AI steps
2. **No schema validation** - Validate input and output with Zod schemas
3. **Hardcoded batch sizes** - Make batch size configurable
4. **Ignoring failed batches** - Log and report failures
5. **No progress tracking** - Use annotations for visibility
</Callout>

## Performance Tips

### Parallel Batch Processing

Process independent batches in parallel (with caution):

```typescript
// Process 5 batches in parallel
const parallelism = 5;
const batchGroups = [];

for (let i = 0; i < batches.length; i += parallelism) {
  const group = batches.slice(i, i + parallelism);
  batchGroups.push(group);
}

for (const group of batchGroups) {
  const results = await Promise.all(
    group.map((batch, index) =>
      step.ai(`batch-${index}`, { ... })
    )
  );
  // Process results...
}
```

<Callout type="info">
**Note**: Be mindful of rate limits when processing in parallel.
</Callout>

### Optimize Token Usage

Reduce prompt size for large batches:

```typescript
// Instead of full JSON
const prompt = `Categorize these IDs: ${batch.map(r => r.id).join(', ')}`;

// Then reconstruct full objects
const categories = result.data;
const enriched = batch.map((record, i) => ({
  ...record,
  category: categories[i]
}));
```

## Next Steps

- [Custom Slash Commands](/docs/examples/custom-slash-command) - Create reusable data operations
- [Type-Safe Arguments](/docs/guides/type-safe-arguments) - Define strict input schemas
- [Error Handling](/docs/guides/error-handling) - Robust error management
- [AI Step Reference](/docs/reference/step-types#ai-step) - AI generation API details
